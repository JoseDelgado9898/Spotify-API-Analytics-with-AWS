{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nimport logging\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom datetime import date,timedelta,datetime\nfrom zoneinfo import ZoneInfo\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nlog= glueContext.get_logger()\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: d397699c-65fa-4bb7-9dcf-06e0f67795d0\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session d397699c-65fa-4bb7-9dcf-06e0f67795d0 to get into ready status...\nSession d397699c-65fa-4bb7-9dcf-06e0f67795d0 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Define JSON Schema",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\nschema = StructType(\n [\n     StructField('raw_json',StringType())\n ]\n)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Ad Hoc Create Dataframe from Raw S3 to Silver Table, parses directory name to figure out Date",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#Executions are only Ad-Hoc , if all raw S3 data needs to be loaded to the silver table\n\ndef create_dataframe_historic():\n    current_date = date.today()-timedelta(days=1)\n    directory_path = f's3://spotify-s3-poc/2025*'\n    df = spark.read.option(\"recursiveFileLookup\", \"true\").schema(schema).text(directory_path)\n    df = df.select(\n        get_json_object(col(\"raw_json\"), \"$.name\").alias(\"Name\"),\n        get_json_object(col(\"raw_json\"), \"$.followers.total\").alias(\"Followers\"))\n    df = df.withColumn('Date',(input_file_name()[21:10]))\n    df.select('date').distinct().show();\n    return df\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create Dataframe with today's data ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def create_dataframe_todays_data():\n    try:\n        tz = ZoneInfo(\"America/Costa_Rica\")\n        current_date = datetime.now(tz).date()\n        directory_path = f\"s3://spotify-s3-poc/{current_date}\"\n        log.info(f'Reading from: {directory_path}')\n        df = spark.read.option(\"recursiveFileLookup\", \"true\").schema(schema).text(directory_path)\n        df = df.select(\n            get_json_object(col(\"raw_json\"), \"$.name\").alias(\"Name\"),\n            get_json_object(col(\"raw_json\"), \"$.followers.total\").alias(\"Followers\"))\n        df = df.withColumn('Date',lit(current_date))\n        log.info(f'Dataframe record count: {df.count()}')\n        return df\n    except Exception as e:\n        log.error(f'Following error encountered: {e}')\n        \n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write DataFrame as Parquet to Silver table, overwrite todays partition to avoid data duplication",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "def write_silver(df):\n    try:\n        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n        (df\n         .write\n         .partitionBy('Date')\n         .mode(\"append\")\n         .format('parquet')\n         .option(\"path\",'s3://spotify-s3-poc/silver/silver_glue_table')\n         .saveAsTable(\"silver_glue_table\")\n        )\n        logging.info('Silver table sucessfully populated with todays data')\n    except Exception as e:\n        logging.error(f'Unexpected error when writting to the silver table: {e}')\n        \ndf = create_dataframe_todays_data();\nwrite_silver(df)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		}
	]
}